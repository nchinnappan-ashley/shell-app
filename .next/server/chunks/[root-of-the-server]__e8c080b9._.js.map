{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 3, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 55, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/NChinnappan/SourceAI/UI/shell-app/src/lib/rag/chunk.ts"],"sourcesContent":["export type Chunk = { id: string; url: string; title?: string; text: string };\n\n// Very lightweight chunker: split by paragraphs/headings, keep ~800-1200 chars per chunk\nexport function chunkText(input: string, url: string, title?: string, maxChars = 1000): Chunk[] {\n  const cleaned = normalizeWhitespace(input);\n  const blocks = cleaned.split(/\\n{2,}/g).map(b => b.trim()).filter(Boolean);\n  const chunks: Chunk[] = [];\n  let buf: string[] = [];\n  let size = 0;\n  const flush = () => {\n    if (!buf.length) return;\n    const text = buf.join(\"\\n\").trim();\n    if (text) chunks.push({ id: `${url}#${chunks.length + 1}`.slice(0, 200), url, title, text });\n    buf = [];\n    size = 0;\n  };\n  for (const block of blocks) {\n    if (size + block.length + 1 > maxChars && size > 0) {\n      flush();\n    }\n    buf.push(block);\n    size += block.length + 1;\n  }\n  flush();\n  return chunks;\n}\n\nexport function normalizeWhitespace(s: string): string {\n  return s\n    .replace(/\\r/g, \"\\n\")\n    .replace(/\\t/g, \" \")\n    .replace(/[\\u00A0\\u2000-\\u200B]/g, \" \")\n    .replace(/\\n{3,}/g, \"\\n\\n\")\n    .replace(/[ \\t]{2,}/g, \" \")\n    .trim();\n}\n\n"],"names":[],"mappings":";;;;;;AAGO,SAAS,UAAU,KAAa,EAAE,GAAW,EAAE,KAAc,EAAE,WAAW,IAAI;IACnF,MAAM,UAAU,oBAAoB;IACpC,MAAM,SAAS,QAAQ,KAAK,CAAC,WAAW,GAAG,CAAC,CAAA,IAAK,EAAE,IAAI,IAAI,MAAM,CAAC;IAClE,MAAM,SAAkB,EAAE;IAC1B,IAAI,MAAgB,EAAE;IACtB,IAAI,OAAO;IACX,MAAM,QAAQ;QACZ,IAAI,CAAC,IAAI,MAAM,EAAE;QACjB,MAAM,OAAO,IAAI,IAAI,CAAC,MAAM,IAAI;QAChC,IAAI,MAAM,OAAO,IAAI,CAAC;YAAE,IAAI,GAAG,IAAI,CAAC,EAAE,OAAO,MAAM,GAAG,GAAG,CAAC,KAAK,CAAC,GAAG;YAAM;YAAK;YAAO;QAAK;QAC1F,MAAM,EAAE;QACR,OAAO;IACT;IACA,KAAK,MAAM,SAAS,OAAQ;QAC1B,IAAI,OAAO,MAAM,MAAM,GAAG,IAAI,YAAY,OAAO,GAAG;YAClD;QACF;QACA,IAAI,IAAI,CAAC;QACT,QAAQ,MAAM,MAAM,GAAG;IACzB;IACA;IACA,OAAO;AACT;AAEO,SAAS,oBAAoB,CAAS;IAC3C,OAAO,EACJ,OAAO,CAAC,OAAO,MACf,OAAO,CAAC,OAAO,KACf,OAAO,CAAC,0BAA0B,KAClC,OAAO,CAAC,WAAW,QACnB,OAAO,CAAC,cAAc,KACtB,IAAI;AACT","debugId":null}},
    {"offset": {"line": 96, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/NChinnappan/SourceAI/UI/shell-app/src/lib/rag/store.ts"],"sourcesContent":["export type StoredChunk = { id: string; url: string; title?: string; text: string };\n\n// Sparse BM25/TF-IDF store (no external dependencies)\n\ntype SparseDoc = {\n  id: string;\n  url: string;\n  title?: string;\n  text: string;\n  length: number; // token count\n};\n\ntype Store = {\n  docs: Map<string, SparseDoc>;\n  // inverted index: term -> map(docId -> term frequency in doc)\n  inv: Map<string, Map<string, number>>;\n  // document frequencies: term -> number of docs containing term\n  df: Map<string, number>;\n  // total docs and average doc length\n  N: number;\n  avgdl: number;\n};\n\n// Global in-memory store (persists for server process lifetime)\nconst g = globalThis as unknown as { __RAG_SPARSE_STORE?: Store };\nif (!g.__RAG_SPARSE_STORE) {\n  g.__RAG_SPARSE_STORE = { docs: new Map(), inv: new Map(), df: new Map(), N: 0, avgdl: 0 } as Store;\n}\nconst store: Store = g.__RAG_SPARSE_STORE as Store;\n\nfunction normalize(s: string) {\n  return s.toLowerCase().replace(/[^a-z0-9 ]/g, ' ').replace(/\\s+/g, ' ').trim();\n}\nfunction tokenize(text: string): string[] {\n  return normalize(text).split(' ').filter(Boolean);\n}\n\nexport function addSparseDocuments(items: StoredChunk[]) {\n  for (const it of items) {\n    const tokens = tokenize(it.text);\n    const tf = new Map<string, number>();\n    const seen = new Set<string>();\n    for (const t of tokens) {\n      tf.set(t, (tf.get(t) || 0) + 1);\n      if (!seen.has(t)) {\n        seen.add(t);\n        store.df.set(t, (store.df.get(t) || 0) + 1);\n      }\n    }\n    // write to inverted index\n    tf.forEach((freq, term) => {\n      let postings = store.inv.get(term);\n      if (!postings) { postings = new Map(); store.inv.set(term, postings); }\n      postings.set(it.id, freq);\n    });\n    // add doc\n    store.docs.set(it.id, { id: it.id, url: it.url, title: it.title, text: it.text, length: tokens.length });\n  }\n  // update corpus stats\n  store.N = store.docs.size;\n  let totalLen = 0;\n  store.docs.forEach(d => totalLen += d.length);\n  store.avgdl = store.N ? totalLen / store.N : 0;\n}\n\nexport function resetStore() {\n  store.docs.clear();\n  store.inv.clear();\n  store.df.clear();\n  store.N = 0;\n  store.avgdl = 0;\n}\n\nexport function size() {\n  return { count: store.docs.size, avgdl: store.avgdl };\n}\n\nexport function listDocs(limit = 50) {\n  const arr: { id: string; url: string; title?: string; length: number }[] = [];\n  store.docs.forEach((d) => arr.push({ id: d.id, url: d.url, title: d.title, length: d.length }));\n  return arr.slice(0, Math.max(0, limit));\n}\n\n// BM25 scoring\nexport function searchSparse(query: string, k = 5, k1 = 1.5, b = 0.75) {\n  const qTokens = tokenize(query);\n  const qTerms = Array.from(new Set(qTokens));\n  const scores = new Map<string, number>();\n  for (const term of qTerms) {\n    const df = store.df.get(term) || 0;\n    if (df === 0 || store.N === 0) continue;\n    const idf = Math.log(1 + (store.N - df + 0.5) / (df + 0.5));\n    const postings = store.inv.get(term)!;\n    postings?.forEach((tf, docId) => {\n      const d = store.docs.get(docId)!;\n      const denom = tf + k1 * (1 - b + b * (d.length / (store.avgdl || 1)));\n      const scoreAdd = idf * ((tf * (k1 + 1)) / (denom || 1));\n      scores.set(docId, (scores.get(docId) || 0) + scoreAdd);\n    });\n  }\n  const ranked = Array.from(scores.entries())\n    .sort((a, b2) => b2[1] - a[1])\n    .slice(0, k)\n    .map(([docId, s]) => {\n      const d = store.docs.get(docId)!;\n      return { id: d.id, url: d.url, title: d.title, text: d.text, score: s };\n    });\n  return ranked;\n}\n"],"names":[],"mappings":";;;;;;;;;;;;AAuBA,gEAAgE;AAChE,MAAM,IAAI;AACV,IAAI,CAAC,EAAE,kBAAkB,EAAE;IACzB,EAAE,kBAAkB,GAAG;QAAE,MAAM,IAAI;QAAO,KAAK,IAAI;QAAO,IAAI,IAAI;QAAO,GAAG;QAAG,OAAO;IAAE;AAC1F;AACA,MAAM,QAAe,EAAE,kBAAkB;AAEzC,SAAS,UAAU,CAAS;IAC1B,OAAO,EAAE,WAAW,GAAG,OAAO,CAAC,eAAe,KAAK,OAAO,CAAC,QAAQ,KAAK,IAAI;AAC9E;AACA,SAAS,SAAS,IAAY;IAC5B,OAAO,UAAU,MAAM,KAAK,CAAC,KAAK,MAAM,CAAC;AAC3C;AAEO,SAAS,mBAAmB,KAAoB;IACrD,KAAK,MAAM,MAAM,MAAO;QACtB,MAAM,SAAS,SAAS,GAAG,IAAI;QAC/B,MAAM,KAAK,IAAI;QACf,MAAM,OAAO,IAAI;QACjB,KAAK,MAAM,KAAK,OAAQ;YACtB,GAAG,GAAG,CAAC,GAAG,CAAC,GAAG,GAAG,CAAC,MAAM,CAAC,IAAI;YAC7B,IAAI,CAAC,KAAK,GAAG,CAAC,IAAI;gBAChB,KAAK,GAAG,CAAC;gBACT,MAAM,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,MAAM,EAAE,CAAC,GAAG,CAAC,MAAM,CAAC,IAAI;YAC3C;QACF;QACA,0BAA0B;QAC1B,GAAG,OAAO,CAAC,CAAC,MAAM;YAChB,IAAI,WAAW,MAAM,GAAG,CAAC,GAAG,CAAC;YAC7B,IAAI,CAAC,UAAU;gBAAE,WAAW,IAAI;gBAAO,MAAM,GAAG,CAAC,GAAG,CAAC,MAAM;YAAW;YACtE,SAAS,GAAG,CAAC,GAAG,EAAE,EAAE;QACtB;QACA,UAAU;QACV,MAAM,IAAI,CAAC,GAAG,CAAC,GAAG,EAAE,EAAE;YAAE,IAAI,GAAG,EAAE;YAAE,KAAK,GAAG,GAAG;YAAE,OAAO,GAAG,KAAK;YAAE,MAAM,GAAG,IAAI;YAAE,QAAQ,OAAO,MAAM;QAAC;IACxG;IACA,sBAAsB;IACtB,MAAM,CAAC,GAAG,MAAM,IAAI,CAAC,IAAI;IACzB,IAAI,WAAW;IACf,MAAM,IAAI,CAAC,OAAO,CAAC,CAAA,IAAK,YAAY,EAAE,MAAM;IAC5C,MAAM,KAAK,GAAG,MAAM,CAAC,GAAG,WAAW,MAAM,CAAC,GAAG;AAC/C;AAEO,SAAS;IACd,MAAM,IAAI,CAAC,KAAK;IAChB,MAAM,GAAG,CAAC,KAAK;IACf,MAAM,EAAE,CAAC,KAAK;IACd,MAAM,CAAC,GAAG;IACV,MAAM,KAAK,GAAG;AAChB;AAEO,SAAS;IACd,OAAO;QAAE,OAAO,MAAM,IAAI,CAAC,IAAI;QAAE,OAAO,MAAM,KAAK;IAAC;AACtD;AAEO,SAAS,SAAS,QAAQ,EAAE;IACjC,MAAM,MAAqE,EAAE;IAC7E,MAAM,IAAI,CAAC,OAAO,CAAC,CAAC,IAAM,IAAI,IAAI,CAAC;YAAE,IAAI,EAAE,EAAE;YAAE,KAAK,EAAE,GAAG;YAAE,OAAO,EAAE,KAAK;YAAE,QAAQ,EAAE,MAAM;QAAC;IAC5F,OAAO,IAAI,KAAK,CAAC,GAAG,KAAK,GAAG,CAAC,GAAG;AAClC;AAGO,SAAS,aAAa,KAAa,EAAE,IAAI,CAAC,EAAE,KAAK,GAAG,EAAE,IAAI,IAAI;IACnE,MAAM,UAAU,SAAS;IACzB,MAAM,SAAS,MAAM,IAAI,CAAC,IAAI,IAAI;IAClC,MAAM,SAAS,IAAI;IACnB,KAAK,MAAM,QAAQ,OAAQ;QACzB,MAAM,KAAK,MAAM,EAAE,CAAC,GAAG,CAAC,SAAS;QACjC,IAAI,OAAO,KAAK,MAAM,CAAC,KAAK,GAAG;QAC/B,MAAM,MAAM,KAAK,GAAG,CAAC,IAAI,CAAC,MAAM,CAAC,GAAG,KAAK,GAAG,IAAI,CAAC,KAAK,GAAG;QACzD,MAAM,WAAW,MAAM,GAAG,CAAC,GAAG,CAAC;QAC/B,UAAU,QAAQ,CAAC,IAAI;YACrB,MAAM,IAAI,MAAM,IAAI,CAAC,GAAG,CAAC;YACzB,MAAM,QAAQ,KAAK,KAAK,CAAC,IAAI,IAAI,IAAI,CAAC,EAAE,MAAM,GAAG,CAAC,MAAM,KAAK,IAAI,CAAC,CAAC,CAAC;YACpE,MAAM,WAAW,MAAM,CAAC,AAAC,KAAK,CAAC,KAAK,CAAC,IAAK,CAAC,SAAS,CAAC,CAAC;YACtD,OAAO,GAAG,CAAC,OAAO,CAAC,OAAO,GAAG,CAAC,UAAU,CAAC,IAAI;QAC/C;IACF;IACA,MAAM,SAAS,MAAM,IAAI,CAAC,OAAO,OAAO,IACrC,IAAI,CAAC,CAAC,GAAG,KAAO,EAAE,CAAC,EAAE,GAAG,CAAC,CAAC,EAAE,EAC5B,KAAK,CAAC,GAAG,GACT,GAAG,CAAC,CAAC,CAAC,OAAO,EAAE;QACd,MAAM,IAAI,MAAM,IAAI,CAAC,GAAG,CAAC;QACzB,OAAO;YAAE,IAAI,EAAE,EAAE;YAAE,KAAK,EAAE,GAAG;YAAE,OAAO,EAAE,KAAK;YAAE,MAAM,EAAE,IAAI;YAAE,OAAO;QAAE;IACxE;IACF,OAAO;AACT","debugId":null}},
    {"offset": {"line": 229, "column": 0}, "map": {"version":3,"sources":["file:///C:/Users/NChinnappan/SourceAI/UI/shell-app/src/app/api/rag/ingest/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\nimport { chunkText } from '@/lib/rag/chunk';\nimport { addSparseDocuments } from '@/lib/rag/store';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\n\nexport const runtime = 'nodejs';\n\nfunction stripHtml(html: string): string {\n  // quick-and-dirty HTML -> text\n  return html\n    .replace(/<script[\\s\\S]*?<\\/script>/gi, '')\n    .replace(/<style[\\s\\S]*?<\\/style>/gi, '')\n    .replace(/<[^>]+>/g, '\\n')\n    .replace(/&nbsp;/g, ' ')\n    .replace(/&amp;/g, '&')\n    .replace(/&lt;/g, '<')\n    .replace(/&gt;/g, '>')\n    .replace(/\\n{3,}/g, '\\n\\n')\n    .trim();\n}\n\nasync function fetchAsText(url: string) {\n  const res = await fetch(url, { headers: { 'User-Agent': 'Ashley-AI-RAG/1.0' } });\n  if (!res.ok) throw new Error(`Failed to fetch ${url}: ${res.status} ${res.statusText}`);\n  const ct = res.headers.get('content-type') || '';\n  const data = await res.text();\n  if (ct.includes('html')) return stripHtml(data);\n  return data;\n}\n\nfunction detectEncoding(buf: Buffer): 'utf8' | 'utf16le' | 'utf16be' {\n  if (buf.length >= 2) {\n    if (buf[0] === 0xFF && buf[1] === 0xFE) return 'utf16le';\n    if (buf[0] === 0xFE && buf[1] === 0xFF) return 'utf16be';\n  }\n  return 'utf8';\n}\nfunction decodeUtf16be(buf: Buffer): string {\n  const swapped = Buffer.allocUnsafe(buf.length);\n  for (let i = 0; i + 1 < buf.length; i += 2) { swapped[i] = buf[i + 1]; swapped[i + 1] = buf[i]; }\n  return swapped.toString('utf16le');\n}\n\nasync function readFilesFromDir(dir: string): Promise<{ url: string; title: string; text: string }[]> {\n  const entries = await fs.readdir(dir, { withFileTypes: true });\n  const out: { url: string; title: string; text: string }[] = [];\n  const allowed = new Set(['.txt', '.md', '.markdown', '.csv', '.json', '.html', '.htm', '.ts', '.tsx', '.js', '.jsx', '.css', '.scss']);\n  for (const ent of entries) {\n    const p = path.join(dir, ent.name);\n    if (ent.isDirectory()) {\n      const nested = await readFilesFromDir(p);\n      out.push(...nested);\n      continue;\n    }\n    const ext = path.extname(ent.name).toLowerCase();\n    if (!allowed.has(ext)) continue;\n    try {\n      const rawBuf = await fs.readFile(p);\n      const enc = detectEncoding(rawBuf);\n      let raw = enc === 'utf8' ? rawBuf.toString('utf8') : (enc === 'utf16le' ? rawBuf.toString('utf16le') : decodeUtf16be(rawBuf));\n      let text = raw;\n      if (ext === '.html' || ext === '.htm') text = stripHtml(raw);\n      else if (ext === '.json') {\n        try { const obj = JSON.parse(raw); text = JSON.stringify(obj, null, 2); } catch {}\n      }\n      const fileUrl = 'file:///' + p.replace(/\\\\/g, '/');\n      out.push({ url: fileUrl, title: path.basename(p), text });\n    } catch {\n      // skip unreadable files\n    }\n  }\n  return out;\n}\n\nexport async function POST(req: NextRequest) {\n  try {\n    const body = await req.json().catch(() => ({}));\n    const urls: string[] = body?.urls || [];\n    const dir: string | undefined = body?.dir;\n    const files: string[] = Array.isArray(body?.files) ? body.files : [];\n    if ((!Array.isArray(urls) || urls.length === 0) && !dir && files.length === 0) {\n      return NextResponse.json({ error: 'Provide one of: { urls: string[] } or { dir: string } or { files: string[] }' }, { status: 400 });\n    }\n\n    const results: { url: string; chunks?: number; error?: string }[] = [];\n\n    // Ingest URLs\n    for (const url of urls) {\n      try {\n        const text = await fetchAsText(url);\n        const chunks = chunkText(text, url, undefined, 1200);\n        addSparseDocuments(chunks);\n        results.push({ url, chunks: chunks.length });\n      } catch (e: unknown) {\n        const msg = e instanceof Error ? e.message : String(e);\n        results.push({ url, error: msg });\n      }\n    }\n\n    // Ingest directory\n    if (dir) {\n      try {\n        const docs = await readFilesFromDir(dir);\n        for (const d of docs) {\n          const chunks = chunkText(d.text, d.url, d.title, 1200);\n          addSparseDocuments(chunks);\n          results.push({ url: d.url, chunks: chunks.length });\n        }\n      } catch (e: unknown) {\n        const msg = e instanceof Error ? e.message : String(e);\n        results.push({ url: dir, error: msg });\n      }\n    }\n\n    // Ingest individual files\n    for (const fp of files) {\n      try {\n        const docs = await readFilesFromDir(path.dirname(fp));\n        const one = docs.find(d => d.url.endsWith(fp.replace(/\\\\/g, '/')));\n        if (one) {\n          const chunks = chunkText(one.text, one.url, one.title, 1200);\n          addSparseDocuments(chunks);\n          results.push({ url: one.url, chunks: chunks.length });\n        } else {\n          const raw = await fs.readFile(fp, 'utf-8');\n          const ext = path.extname(fp).toLowerCase();\n          let text = raw;\n          if (ext === '.html' || ext === '.htm') text = stripHtml(raw);\n          else if (ext === '.json') { try { text = JSON.stringify(JSON.parse(raw), null, 2); } catch {} }\n          const fileUrl = 'file:///' + fp.replace(/\\\\/g, '/');\n          const chunks = chunkText(text, fileUrl, path.basename(fp), 1200);\n          addSparseDocuments(chunks);\n          results.push({ url: fileUrl, chunks: chunks.length });\n        }\n      } catch (e: unknown) {\n        const msg = e instanceof Error ? e.message : String(e);\n        const fileUrl = 'file:///' + fp.replace(/\\\\/g, '/');\n        results.push({ url: fileUrl, error: msg });\n      }\n    }\n\n    return NextResponse.json({ ok: true, results });\n  } catch (e: unknown) {\n    const msg = e instanceof Error ? e.message : String(e);\n    return NextResponse.json({ error: msg }, { status: 500 });\n  }\n}\n\n"],"names":[],"mappings":";;;;;;AAAA;AACA;AACA;AACA;AACA;;;;;;AAEO,MAAM,UAAU;AAEvB,SAAS,UAAU,IAAY;IAC7B,+BAA+B;IAC/B,OAAO,KACJ,OAAO,CAAC,+BAA+B,IACvC,OAAO,CAAC,6BAA6B,IACrC,OAAO,CAAC,YAAY,MACpB,OAAO,CAAC,WAAW,KACnB,OAAO,CAAC,UAAU,KAClB,OAAO,CAAC,SAAS,KACjB,OAAO,CAAC,SAAS,KACjB,OAAO,CAAC,WAAW,QACnB,IAAI;AACT;AAEA,eAAe,YAAY,GAAW;IACpC,MAAM,MAAM,MAAM,MAAM,KAAK;QAAE,SAAS;YAAE,cAAc;QAAoB;IAAE;IAC9E,IAAI,CAAC,IAAI,EAAE,EAAE,MAAM,IAAI,MAAM,CAAC,gBAAgB,EAAE,IAAI,EAAE,EAAE,IAAI,MAAM,CAAC,CAAC,EAAE,IAAI,UAAU,EAAE;IACtF,MAAM,KAAK,IAAI,OAAO,CAAC,GAAG,CAAC,mBAAmB;IAC9C,MAAM,OAAO,MAAM,IAAI,IAAI;IAC3B,IAAI,GAAG,QAAQ,CAAC,SAAS,OAAO,UAAU;IAC1C,OAAO;AACT;AAEA,SAAS,eAAe,GAAW;IACjC,IAAI,IAAI,MAAM,IAAI,GAAG;QACnB,IAAI,GAAG,CAAC,EAAE,KAAK,QAAQ,GAAG,CAAC,EAAE,KAAK,MAAM,OAAO;QAC/C,IAAI,GAAG,CAAC,EAAE,KAAK,QAAQ,GAAG,CAAC,EAAE,KAAK,MAAM,OAAO;IACjD;IACA,OAAO;AACT;AACA,SAAS,cAAc,GAAW;IAChC,MAAM,UAAU,OAAO,WAAW,CAAC,IAAI,MAAM;IAC7C,IAAK,IAAI,IAAI,GAAG,IAAI,IAAI,IAAI,MAAM,EAAE,KAAK,EAAG;QAAE,OAAO,CAAC,EAAE,GAAG,GAAG,CAAC,IAAI,EAAE;QAAE,OAAO,CAAC,IAAI,EAAE,GAAG,GAAG,CAAC,EAAE;IAAE;IAChG,OAAO,QAAQ,QAAQ,CAAC;AAC1B;AAEA,eAAe,iBAAiB,GAAW;IACzC,MAAM,UAAU,MAAM,gJAAE,CAAC,OAAO,CAAC,KAAK;QAAE,eAAe;IAAK;IAC5D,MAAM,MAAsD,EAAE;IAC9D,MAAM,UAAU,IAAI,IAAI;QAAC;QAAQ;QAAO;QAAa;QAAQ;QAAS;QAAS;QAAQ;QAAO;QAAQ;QAAO;QAAQ;QAAQ;KAAQ;IACrI,KAAK,MAAM,OAAO,QAAS;QACzB,MAAM,IAAI,4HAAI,CAAC,IAAI,CAAC,KAAK,IAAI,IAAI;QACjC,IAAI,IAAI,WAAW,IAAI;YACrB,MAAM,SAAS,MAAM,iBAAiB;YACtC,IAAI,IAAI,IAAI;YACZ;QACF;QACA,MAAM,MAAM,4HAAI,CAAC,OAAO,CAAC,IAAI,IAAI,EAAE,WAAW;QAC9C,IAAI,CAAC,QAAQ,GAAG,CAAC,MAAM;QACvB,IAAI;YACF,MAAM,SAAS,MAAM,gJAAE,CAAC,QAAQ,CAAC;YACjC,MAAM,MAAM,eAAe;YAC3B,IAAI,MAAM,QAAQ,SAAS,OAAO,QAAQ,CAAC,UAAW,QAAQ,YAAY,OAAO,QAAQ,CAAC,aAAa,cAAc;YACrH,IAAI,OAAO;YACX,IAAI,QAAQ,WAAW,QAAQ,QAAQ,OAAO,UAAU;iBACnD,IAAI,QAAQ,SAAS;gBACxB,IAAI;oBAAE,MAAM,MAAM,KAAK,KAAK,CAAC;oBAAM,OAAO,KAAK,SAAS,CAAC,KAAK,MAAM;gBAAI,EAAE,OAAM,CAAC;YACnF;YACA,MAAM,UAAU,aAAa,EAAE,OAAO,CAAC,OAAO;YAC9C,IAAI,IAAI,CAAC;gBAAE,KAAK;gBAAS,OAAO,4HAAI,CAAC,QAAQ,CAAC;gBAAI;YAAK;QACzD,EAAE,OAAM;QACN,wBAAwB;QAC1B;IACF;IACA,OAAO;AACT;AAEO,eAAe,KAAK,GAAgB;IACzC,IAAI;QACF,MAAM,OAAO,MAAM,IAAI,IAAI,GAAG,KAAK,CAAC,IAAM,CAAC,CAAC,CAAC;QAC7C,MAAM,OAAiB,MAAM,QAAQ,EAAE;QACvC,MAAM,MAA0B,MAAM;QACtC,MAAM,QAAkB,MAAM,OAAO,CAAC,MAAM,SAAS,KAAK,KAAK,GAAG,EAAE;QACpE,IAAI,CAAC,CAAC,MAAM,OAAO,CAAC,SAAS,KAAK,MAAM,KAAK,CAAC,KAAK,CAAC,OAAO,MAAM,MAAM,KAAK,GAAG;YAC7E,OAAO,gKAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAA+E,GAAG;gBAAE,QAAQ;YAAI;QACpI;QAEA,MAAM,UAA8D,EAAE;QAEtE,cAAc;QACd,KAAK,MAAM,OAAO,KAAM;YACtB,IAAI;gBACF,MAAM,OAAO,MAAM,YAAY;gBAC/B,MAAM,SAAS,IAAA,yJAAS,EAAC,MAAM,KAAK,WAAW;gBAC/C,IAAA,kKAAkB,EAAC;gBACnB,QAAQ,IAAI,CAAC;oBAAE;oBAAK,QAAQ,OAAO,MAAM;gBAAC;YAC5C,EAAE,OAAO,GAAY;gBACnB,MAAM,MAAM,aAAa,QAAQ,EAAE,OAAO,GAAG,OAAO;gBACpD,QAAQ,IAAI,CAAC;oBAAE;oBAAK,OAAO;gBAAI;YACjC;QACF;QAEA,mBAAmB;QACnB,IAAI,KAAK;YACP,IAAI;gBACF,MAAM,OAAO,MAAM,iBAAiB;gBACpC,KAAK,MAAM,KAAK,KAAM;oBACpB,MAAM,SAAS,IAAA,yJAAS,EAAC,EAAE,IAAI,EAAE,EAAE,GAAG,EAAE,EAAE,KAAK,EAAE;oBACjD,IAAA,kKAAkB,EAAC;oBACnB,QAAQ,IAAI,CAAC;wBAAE,KAAK,EAAE,GAAG;wBAAE,QAAQ,OAAO,MAAM;oBAAC;gBACnD;YACF,EAAE,OAAO,GAAY;gBACnB,MAAM,MAAM,aAAa,QAAQ,EAAE,OAAO,GAAG,OAAO;gBACpD,QAAQ,IAAI,CAAC;oBAAE,KAAK;oBAAK,OAAO;gBAAI;YACtC;QACF;QAEA,0BAA0B;QAC1B,KAAK,MAAM,MAAM,MAAO;YACtB,IAAI;gBACF,MAAM,OAAO,MAAM,iBAAiB,4HAAI,CAAC,OAAO,CAAC;gBACjD,MAAM,MAAM,KAAK,IAAI,CAAC,CAAA,IAAK,EAAE,GAAG,CAAC,QAAQ,CAAC,GAAG,OAAO,CAAC,OAAO;gBAC5D,IAAI,KAAK;oBACP,MAAM,SAAS,IAAA,yJAAS,EAAC,IAAI,IAAI,EAAE,IAAI,GAAG,EAAE,IAAI,KAAK,EAAE;oBACvD,IAAA,kKAAkB,EAAC;oBACnB,QAAQ,IAAI,CAAC;wBAAE,KAAK,IAAI,GAAG;wBAAE,QAAQ,OAAO,MAAM;oBAAC;gBACrD,OAAO;oBACL,MAAM,MAAM,MAAM,gJAAE,CAAC,QAAQ,CAAC,IAAI;oBAClC,MAAM,MAAM,4HAAI,CAAC,OAAO,CAAC,IAAI,WAAW;oBACxC,IAAI,OAAO;oBACX,IAAI,QAAQ,WAAW,QAAQ,QAAQ,OAAO,UAAU;yBACnD,IAAI,QAAQ,SAAS;wBAAE,IAAI;4BAAE,OAAO,KAAK,SAAS,CAAC,KAAK,KAAK,CAAC,MAAM,MAAM;wBAAI,EAAE,OAAM,CAAC;oBAAE;oBAC9F,MAAM,UAAU,aAAa,GAAG,OAAO,CAAC,OAAO;oBAC/C,MAAM,SAAS,IAAA,yJAAS,EAAC,MAAM,SAAS,4HAAI,CAAC,QAAQ,CAAC,KAAK;oBAC3D,IAAA,kKAAkB,EAAC;oBACnB,QAAQ,IAAI,CAAC;wBAAE,KAAK;wBAAS,QAAQ,OAAO,MAAM;oBAAC;gBACrD;YACF,EAAE,OAAO,GAAY;gBACnB,MAAM,MAAM,aAAa,QAAQ,EAAE,OAAO,GAAG,OAAO;gBACpD,MAAM,UAAU,aAAa,GAAG,OAAO,CAAC,OAAO;gBAC/C,QAAQ,IAAI,CAAC;oBAAE,KAAK;oBAAS,OAAO;gBAAI;YAC1C;QACF;QAEA,OAAO,gKAAY,CAAC,IAAI,CAAC;YAAE,IAAI;YAAM;QAAQ;IAC/C,EAAE,OAAO,GAAY;QACnB,MAAM,MAAM,aAAa,QAAQ,EAAE,OAAO,GAAG,OAAO;QACpD,OAAO,gKAAY,CAAC,IAAI,CAAC;YAAE,OAAO;QAAI,GAAG;YAAE,QAAQ;QAAI;IACzD;AACF","debugId":null}}]
}